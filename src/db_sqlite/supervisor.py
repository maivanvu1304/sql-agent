# from langchain_openai import ChatOpenAI
# from langchain_core.messages import HumanMessage, ToolMessage, AIMessage, SystemMessage
# from langgraph.prebuilt import ToolNode
# from langgraph.graph import StateGraph, END, MessagesState
# from typing import Literal, TypedDict, Annotated, Sequence
# from operator import add
# from langchain_core.messages import BaseMessage
# from src.db_sqlite.graph import agent as sql_agent
# from src.db_sqlite.agent_tool import create_sql_agent_tool

# sql_agent_tool = create_sql_agent_tool(sql_agent)

# from langchain_community.tools.tavily_search import TavilySearchResults
# web_search_tool = TavilySearchResults(max_results=2, name="web_search")

# tools = [sql_agent_tool, web_search_tool]
# tool_node = ToolNode(tools)

# supervisor_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
# model_with_tools = supervisor_llm.bind_tools(tools)

# # def supervisor_node(state: MessagesState) -> dict:
# #     """Main supervisor node: decides the next action."""
# #     response = model_with_tools.invoke(state['messages'])
# #     return {"messages": [response]}

# class CustomAgentState(TypedDict):
#     messages: Annotated[Sequence[BaseMessage], add]
#     tool: str | None  # Tool ƒë∆∞·ª£c ch·ªçn t·ª´ frontend


# def supervisor_node(state: CustomAgentState) -> dict:
#     """
#     Main supervisor node: decides the next action.
    
#     Logic:
#     1. N·∫øu user ƒë√£ ch·ªçn tool ‚Üí B·∫ÆT BU·ªòC d√πng tool ƒë√≥
#     2. N·∫øu tool kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c ‚Üí Th√¥ng b√°o r√µ r√†ng
#     3. N·∫øu kh√¥ng ch·ªçn tool ‚Üí ƒê·ªÉ LLM t·ª± quy·∫øt ƒë·ªãnh
#     """
    
#     preferred_tool = state.get('tool')
    
#     # ===== CASE 1: User ƒë√£ ch·ªçn tool c·ª• th·ªÉ =====
#     if preferred_tool:
#         print(f"üîí User ch·ªçn tool: {preferred_tool}")
        
#         # Ki·ªÉm tra tool c√≥ t·ªìn t·∫°i kh√¥ng
#         tool_info = next((t for t in tools if t.name == preferred_tool), None)
        
#         if not tool_info:
#             # Tool kh√¥ng t·ªìn t·∫°i ‚Üí Tr·∫£ v·ªÅ error message
#             error_msg = AIMessage(
#                 content=f"‚ùå Kh√¥ng t√¨m th·∫•y tool '{preferred_tool}'. Vui l√≤ng ch·ªçn tool kh√°c t·ª´ danh s√°ch c√≥ s·∫µn."
#             )
#             return {"messages": [error_msg]}
        
#         try:
#             # üî• B·∫ÆT BU·ªòC s·ª≠ d·ª•ng tool ƒë∆∞·ª£c ch·ªçn
#             model_with_forced_tool = supervisor_llm.bind_tools(
#                 tools,
#                 tool_choice={"type": "function", "function": {"name": preferred_tool}}
#             )
            
#             # T·∫°o instruction cho model
#             instruction = f"""B·∫°n PH·∫¢I s·ª≠ d·ª•ng tool '{preferred_tool}' ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.

# Tool: {tool_info.name}
# M√¥ t·∫£: {tool_info.description}

# N·∫øu tool n√†y kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, h√£y n√≥i r√µ l√Ω do v√† ƒë·ªÅ xu·∫•t tool ph√π h·ª£p h∆°n."""
            
#             messages_with_instruction = [
#                 SystemMessage(content=instruction)
#             ] + state['messages']
            
#             # G·ªçi model v·ªõi tool b·∫Øt bu·ªôc
#             response = model_with_forced_tool.invoke(messages_with_instruction)
            
#             # Verify tool ƒë√£ ƒë∆∞·ª£c g·ªçi
#             if hasattr(response, 'tool_calls') and response.tool_calls:
#                 used_tool = response.tool_calls[0]['name']
#                 print(f"‚úÖ Tool ƒë∆∞·ª£c s·ª≠ d·ª•ng: {used_tool}")
                
#                 if used_tool == preferred_tool:
#                     return {"messages": [response]}
#                 else:
#                     print(f"‚ö†Ô∏è Warning: Model d√πng {used_tool} thay v√¨ {preferred_tool}")
            
#             return {"messages": [response]}
            
#         except Exception as e:
#             # X·ª≠ l√Ω l·ªói khi force tool
#             print(f"‚ùå L·ªói khi s·ª≠ d·ª•ng tool '{preferred_tool}': {e}")
#             error_msg = AIMessage(
#                 content=f"‚ùå Kh√¥ng th·ªÉ s·ª≠ d·ª•ng tool '{preferred_tool}' ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.\n\n"
#                         f"L√Ω do: {str(e)}\n\n"
#                         f"üí° ƒê·ªÅ xu·∫•t: Vui l√≤ng ch·ªçn tool kh√°c ph√π h·ª£p h∆°n v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."
#             )
#             return {"messages": [error_msg]}
    
#     # ===== CASE 2: Kh√¥ng ch·ªçn tool ‚Üí LLM t·ª± quy·∫øt ƒë·ªãnh =====
#     else:
#         print("ü§ñ Kh√¥ng c√≥ tool preference - LLM t·ª± quy·∫øt ƒë·ªãnh")
#         response = model_with_tools.invoke(state['messages'])
#         return {"messages": [response]}


# def should_continue(state: MessagesState) -> Literal["continue", "end"]:
#     """Determines whether to continue to tools or end."""
#     last_message = state['messages'][-1]
#     if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
#         return "continue"
#     return "end"

# builder = StateGraph(MessagesState)

# builder.add_node("supervisor", supervisor_node)
# builder.add_node("tools", tool_node)

# builder.set_entry_point("supervisor")

# builder.add_conditional_edges(
#     "supervisor",
#     should_continue,
#     {"continue": "tools", "end": END}
# )
# builder.add_edge("tools", "supervisor")

# supervisor_agent = builder.compile()

# import uvicorn
# from fastapi import FastAPI
# from fastapi.middleware.cors import CORSMiddleware

# app = FastAPI()

# # Add CORS middleware
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# @app.get("/tools")
# async def get_tools():
#     """Endpoint to get the list of tools"""
#     tools_info = []
    
#     for tool in tools:
#         tool_info = {
#             "name": tool.name,
#             "description": tool.description,
#             "args_schema": tool.args_schema.schema() if hasattr(tool, 'args_schema') else None,
#         }
#         tools_info.append(tool_info)
    
#     return tools_info
# if __name__ == "__main__":
#     # Ch·∫°y FastAPI server
#     uvicorn.run(app, host="0.0.0.0", port=8000)

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, ToolMessage, AIMessage, SystemMessage
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, END, MessagesState
from typing import Literal

from src.db_sqlite.graph import agent as sql_agent
from src.db_sqlite.agent_tool import create_sql_agent_tool

sql_agent_tool = create_sql_agent_tool(sql_agent)

from langchain_community.tools.tavily_search import TavilySearchResults
web_search_tool = TavilySearchResults(max_results=2, name="web_search")

tools = [sql_agent_tool, web_search_tool]
tool_node = ToolNode(tools)

supervisor_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
model_with_tools = supervisor_llm.bind_tools(tools)


class CustomAgentState(MessagesState):
    """Custom state with tool preference"""
    tool: str | None = None


def supervisor_node(state: CustomAgentState) -> dict:
    """
    Main supervisor node: decides the next action.
    If the user has selected a tool and it has not been executed, it must be used.
    If the tool has been executed, the result is summarized.
    If no tool is selected, the LLM decides.
    """
    
    preferred_tool = state.get('tool')
    messages = state['messages']
    
    tool_already_called = False
    if preferred_tool and len(messages) > 1:
        for msg in reversed(messages):
            if isinstance(msg, ToolMessage):
                tool_already_called = True
                break
            elif isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
                for tc in msg.tool_calls:
                    if tc.get('name') == preferred_tool:
                        tool_already_called = True
                        break
                if tool_already_called:
                    break
    
    if preferred_tool and tool_already_called:
        print(f"‚úÖ Tool '{preferred_tool}' ƒë√£ ƒë∆∞·ª£c execute, ƒëang t·ªïng h·ª£p k·∫øt qu·∫£...")
        
        response = supervisor_llm.invoke(messages)
        
        return {"messages": [response], "tool": None}
    
    elif preferred_tool:
        print(f"üîí User ch·ªçn tool: {preferred_tool} (l·∫ßn ƒë·∫ßu)")
        
        tool_info = next((t for t in tools if t.name == preferred_tool), None)
        
        if not tool_info:
            error_msg = AIMessage(
                content=f"‚ùå Kh√¥ng t√¨m th·∫•y tool '{preferred_tool}'. Vui l√≤ng ch·ªçn tool kh√°c."
            )
            return {"messages": [error_msg], "tool": None}
        
        try:
            model_with_forced_tool = supervisor_llm.bind_tools(
                tools,
                tool_choice={"type": "function", "function": {"name": preferred_tool}}
            )
            
            instruction = f"""S·ª≠ d·ª•ng tool '{preferred_tool}' ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.

Tool: {tool_info.name}
M√¥ t·∫£: {tool_info.description}"""
            
            messages_with_instruction = [
                SystemMessage(content=instruction)
            ] + messages
            
            response = model_with_forced_tool.invoke(messages_with_instruction)
            
            if hasattr(response, 'tool_calls') and response.tool_calls:
                used_tool = response.tool_calls[0]['name']
                print(f"‚úÖ Tool ƒë∆∞·ª£c s·ª≠ d·ª•ng: {used_tool}")
            
            return {"messages": [response]}
            
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")
            error_msg = AIMessage(
                content=f"‚ùå Kh√¥ng th·ªÉ s·ª≠ d·ª•ng tool '{preferred_tool}'.\n\nL√Ω do: {str(e)}\n\nüí° Vui l√≤ng ch·ªçn tool kh√°c."
            )
            return {"messages": [error_msg], "tool": None}
    
    else:
        print("ü§ñ Kh√¥ng c√≥ tool preference - LLM t·ª± quy·∫øt ƒë·ªãnh")
        response = model_with_tools.invoke(messages)
        return {"messages": [response]}


def should_continue(state: CustomAgentState) -> Literal["continue", "end"]:
    """Determines whether to continue to tools or end."""
    last_message = state['messages'][-1]
    
    # N·∫øu c√≥ tool_calls ‚Üí execute tools
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "continue"
    
    # N·∫øu kh√¥ng c√≥ tool_calls ‚Üí k·∫øt th√∫c
    return "end"


builder = StateGraph(CustomAgentState)

builder.add_node("supervisor", supervisor_node)
builder.add_node("tools", tool_node)

builder.set_entry_point("supervisor")

builder.add_conditional_edges(
    "supervisor",
    should_continue,
    {"continue": "tools", "end": END}
)
builder.add_edge("tools", "supervisor")

supervisor_agent = builder.compile()


# FastAPI
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/tools")
async def get_tools():
    """Endpoint to get the list of tools"""
    tools_info = []
    
    for tool in tools:
        tool_info = {
            "name": tool.name,
            "description": tool.description,
            "args_schema": tool.args_schema.schema() if hasattr(tool, 'args_schema') else None,
        }
        tools_info.append(tool_info)
    
    return tools_info

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)